---
title: "p8105_hw5_zl3386"
author: "Ziqiu Liu"
date: "2023-11-08"
output: github_document
---

```{r, message = FALSE}
library(tidyverse)
library(rvest)
library(readr)
```

## Problem 1

```{r import data}
homicide_df = 
  read_csv("data/homicide-data.csv") |>
  mutate(
    victim_age = as.numeric(victim_age))
```

* The raw data has `r nrow(homicide_df)` rows and `r ncol(homicide_df)` columns. Each row represents a homicide record (identified by a `uid`), with information about the location of the killing (`city`, `state`, `lat` and `lon`), whether an arrest was made (`disposition`), and basic demographic information about the victim (last and first name, race, age and sex).


Create a `city_state` variable.

```{r}
homicide_df = homicide_df |>
  mutate(
    city_state = paste(city, ",", state)
  )
```

summarize within cities to obtain the total number of homicides and the number of unsolved homicides.

```{r}
homicide_df |>
  group_by(city) |>
  summarize(
    total_homicide = n(),
    unsolved_homicide = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
  )
```


For the city of Baltimore, MD, use the `prop.test` function to estimate the proportion of homicides that are unsolved; save the output of `prop.test` as an R object, apply the `broom::tidy` to this object and pull the estimated proportion and confidence intervals from the resulting tidy dataframe.

```{r}
unsolved = homicide_df |>
  group_by(city) |>
  summarize(
    total_homicide = n(),
    unsolved_homicide = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
  ) |>
  filter(city == "Baltimore") |>
  select(unsolved_homicide, total_homicide) 

Baltimore_prop = prop.test(x = pull(unsolved, unsolved_homicide), n = pull(unsolved, total_homicide)) |>
  broom::tidy() |>
  select(estimate, conf.low, conf.high)
```

* The estimated proportion is `r Baltimore_prop |> pull(estimate) |> round(3)`, and its 95% confidence interval is [`r Baltimore_prop |> pull(conf.low) |> round(3)`, `r Baltimore_prop |> pull(conf.high) |> round(3)`].


Run `prop.test` for each of the cities in the dataset, and extract both the proportion of unsolved homicides and the confidence interval for each.

```{r}
tb_prop_test = function(tb, alpha = 0.95) {
  prop.test(x = sum(tb), n = nrow(tb), conf.level = alpha)
}

unsolved_df = homicide_df |>
  mutate(
    unsolved = case_match(
      disposition,
      c("Closed without arrest", "Open/No arrest") ~ 1,
      "Closed by arrest" ~ 0
    )
  ) |>
  select(city, unsolved) |>
  nest(unsolved_status = -city) |>
  mutate(
    test = map(unsolved_status, tb_prop_test),
    results = map(test, broom::tidy)
  ) |>
  select(city, results) |>
  unnest(results) |>
  select(city, estimate, conf.low, conf.high)
  
```

Create a plot that shows the estimates and CIs for each city.

```{r}
unsolved_df |>
  mutate(city = fct_reorder(city, estimate)) |> 
  ggplot(aes(x = city, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  labs(title = "Estimated proportion of unsolved homicides for different cities", y = "proportion") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) 
```


## Problem 2

```{r}
# dataframe containing all file names
file_name = list.files("data/study_data")

# read in data for each subject
read_data = function(file) {
  read_csv(str_c("data/study_data/", file))
}

study_df = data.frame(file_name) |>
  mutate(
    data = map(file_name, read_data)
  )
  
# tidy the result
study_df = study_df |>
  mutate(
    arm = substr(file_name, 1, 3),
    subject_id = substr(file_name, 5, 6)
  ) |>
  select(subject_id, arm, data) |>
  unnest(data) |>
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "value"
  ) |>
  mutate(week = parse_number(week))

```

Make a spaghetti plot showing observations on each subject over time.

```{r}
study_df |> 
  ggplot(aes(x = week, y = value, group = interaction(subject_id, arm), color = subject_id, lty = arm)) +
  geom_line()
```

* From the spaghetti plot, we can see that in this longitudinal study, the experimental arm has a significant higher value than the control arm in general.

* Also, the value in the experimental arm tends to increase over time, while the value in the control arm remains relatively stable.


## Problem 3

```{r}
n = 30
sigma = 5
reps = 5000

# store the results for each mu value
results = tibble(
  mu = numeric(), 
  power = numeric(), 
  reject_mu_hat = numeric(), 
  total_mu_hat = numeric()
)

# function for generating each dataset and obtain the estimate and p-value
sim_normal_test = function(mu = 0, sigma = 5, n = 30) {
  rnorm(n, mu, sigma) |>
    t.test() |>
    broom::tidy() |>
    select(estimate, p.value)
}

# iteration
for(mu in 0:6) {
  reject_num = 0
  reject_est = c()
  total_est = c()
  
  for(i in 1:reps) {
    test = sim_normal_test(mu = mu)
    total_est = append(total_est, pull(test, estimate))
    
    if(pull(test, p.value) < 0.05) {
      reject_num = reject_num + 1
      reject_est = append(reject_est, pull(test, estimate))
    }
    
  }
  
  power = reject_num / reps
  reject_mu_hat = mean(reject_est)
  total_mu_hat = mean(total_est)
  
  results = results |>
    add_row(
      mu = mu,
      power = power,
      reject_mu_hat = reject_mu_hat,
      total_mu_hat = total_mu_hat
    )
  
}

```

Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of μ on the x axis.

```{r}
# power of the test
results |>
  ggplot(aes(x = mu, y = power)) +
  geom_point() +
  geom_line() +
  labs(title = "Power of the test")
```

* From the plot, we can see that there's a positive association between effect size and power, which means when the difference between the true μ and the null value 0 increases, there's a higher probability of rejecting the false null hypothesis.

* Also, as μ gets larger, the increasing of power will slow down and approach the maximum value of 1.


Make a plot showing the average estimate of $\hat{μ}$ on the y axis and the true value of μ on the x axis.

```{r}
results |>
  ggplot(aes(x = mu, y = total_mu_hat)) +
  geom_point() +
  geom_line() +
  labs(title = "Average estimate of mu")
```

Make a second plot showing the average estimate of $\hat{μ}$ __only in samples for which the null was rejected__ on the y axis and the true value of μ on the x axis.

```{r}
results |>
  ggplot(aes(x = mu, y = reject_mu_hat)) +
  geom_point() +
  geom_line() +
  labs(title = "Average estimate of mu in null rejected samples")
```

* From the plot, we can see that the sample average of $\hat{μ}$ across tests for which the null is rejected approximately equal to the true value of μ.

* This is because the samples were randomly generated from x∼Normal[μ,σ], and when the number of samples (and null rejected samples) is large enough, their mean distribution tend to be approximately __symmetric__ to the true μ due to the randomness, causing the average of null rejected samples' $\hat{μ}$ to be approximately equal to μ.